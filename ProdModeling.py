# Databricks notebook source
# MAGIC %md
# MAGIC # DAIS 2021 Data Science session: Modeling
# MAGIC 
# MAGIC Auto ML generated a baseline model for us, but, we could already see it was too simplistic. From that working modeling code, the data scientist could iterate and improve it by hand.
# MAGIC 
# MAGIC ** ... time passes ... **
# MAGIC 
# MAGIC A few days later, we've got a condensed and improved variation on the modeling code generated by AutoML.
# MAGIC 
# MAGIC Get the latest features from the feature store:

# COMMAND ----------

from databricks.feature_store import FeatureStoreClient, FeatureLookup

fs = FeatureStoreClient()

training_set = fs.create_training_set(spark.read.table("seanowen.demographic"), 
                                      [FeatureLookup(table_name = "seanowen.service_features", lookup_key="customerID")], 
                                      label="Churn", exclude_columns="customerID")
df_loaded = training_set.load_df().toPandas()

# COMMAND ----------

# MAGIC %md
# MAGIC This is the same as the code produced by auto ML, to define the model:

# COMMAND ----------

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler
from xgboost import XGBClassifier

def build_model(params):
  transformers = []

  bool_pipeline = Pipeline(steps=[
      ("cast_type", FunctionTransformer(lambda df: df.astype(object))),
      ("imputer", SimpleImputer(missing_values=None, strategy="most_frequent")),
      ("onehot", OneHotEncoder(handle_unknown="ignore")),
  ])
  transformers.append(("boolean", bool_pipeline, 
                       ["Dependents", "PaperlessBilling", "Partner", "PhoneService", "SeniorCitizen"]))

  numerical_pipeline = Pipeline(steps=[
      ("converter", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors="coerce"))),
      ("imputer", SimpleImputer(strategy="mean"))
  ])
  transformers.append(("numerical", numerical_pipeline, 
                       ["AvgPriceIncrease", "Contract", "MonthlyCharges", "NumOptionalServices", "TotalCharges", "tenure"]))

  one_hot_pipeline = Pipeline(steps=[
      ("imputer", SimpleImputer(missing_values=None, strategy="constant", fill_value="")),
      ("onehot", OneHotEncoder(handle_unknown="ignore"))
  ])
  transformers.append(("onehot", one_hot_pipeline, 
                       ["DeviceProtection", "InternetService", "MultipleLines", "OnlineBackup", \
                        "OnlineSecurity", "PaymentMethod", "StreamingMovies", "StreamingTV", "TechSupport", "gender"]))

  xgbc_classifier = XGBClassifier(
    n_estimators=int(params['n_estimators']),
    learning_rate=params['learning_rate'],
    max_depth=int(params['max_depth']),
    min_child_weight=params['min_child_weight'],
    random_state=810302555
  )

  return Pipeline([
      ("preprocessor", ColumnTransformer(transformers, remainder="passthrough", sparse_threshold=0)),
      ("standardizer", StandardScaler()),
      ("classifier", xgbc_classifier),
  ])

# COMMAND ----------

from sklearn.model_selection import train_test_split

target_col = "Churn"
split_X = df_loaded.drop([target_col], axis=1)
split_y = df_loaded[target_col]

X_train, X_val, y_train, y_val = train_test_split(split_X, split_y, train_size=0.9, random_state=810302555, stratify=split_y)

# COMMAND ----------

# MAGIC %md
# MAGIC Here, we use `hyperopt` to perform some 'auto ML' every time the model is rebuilt, to fine tune it. This is similar to what auto ML did to arrive at the initial baseline model.

# COMMAND ----------

from hyperopt import fmin, hp, tpe, SparkTrials, STATUS_OK
import numpy as np
from sklearn.metrics import log_loss, accuracy_score
import mlflow

def train_model(params):
  model = build_model(params)
  model.fit(X_train, y_train)
  loss = log_loss(y_val, model.predict_proba(X_val))
  mlflow.log_metrics({'log_loss': loss, 'accuracy': accuracy_score(y_val, model.predict(X_val))})
  return { 'status': STATUS_OK, 'loss': loss }
  
search_space = {
  'max_depth':        hp.quniform('max_depth', 3, 10, 1),
  'learning_rate':    hp.loguniform('learning_rate', -5, -1),
  'min_child_weight': hp.loguniform('min_child_weight', 0, 2),
  'n_estimators':     hp.quniform('n_estimators', 50, 500, 10)
}

best_params = fmin(fn=train_model, space=search_space, algo=tpe.suggest, \
                   max_evals=64, trials=SparkTrials(parallelism=8), \
                   rstate=np.random.RandomState(810302555))

# COMMAND ----------

# MAGIC %md
# MAGIC Now, build one last model on all the data, with the best hyperparams. The model is logged in a 'feature store aware' way, so that it can perform the joins at runtime. The model doesn't need to be fed the features manually.

# COMMAND ----------

import mlflow
from mlflow.models.signature import infer_signature

mlflow.autolog(log_input_examples=True)

with mlflow.start_run() as run:
  training_set = fs.create_training_set(spark.read.table("seanowen.demographic"), 
                                      [FeatureLookup(table_name = "seanowen.service_features", lookup_key="customerID")], 
                                      label="Churn", exclude_columns="customerID")
  df_loaded = training_set.load_df().toPandas()
  split_X = df_loaded.drop([target_col], axis=1)
  split_y = df_loaded[target_col]

  model = build_model(best_params)
  model.fit(split_X, split_y)
  
  fs.log_model(
    model,
    "model",
    flavor=mlflow.sklearn,
    training_set=training_set,
    registered_model_name="dais-2021-churn",
    input_example=split_X[:100],
    signature=infer_signature(split_X, split_y))
  
  best_run = run.info

# COMMAND ----------

# MAGIC %md
# MAGIC The process above created a new version of the registered model `dais-2021-churn`. Transition it to Staging.

# COMMAND ----------

import mlflow.tracking

client = mlflow.tracking.MlflowClient()

model_version = client.get_latest_versions("dais-2021-churn", stages=["None"])[0]
client.transition_model_version_stage("dais-2021-churn", model_version.version, stage="Staging")

# COMMAND ----------

# MAGIC %md
# MAGIC A webhook was previously set up to trigger an automated testing notebook whenever a new Staging candidate is registered. If successful, the model is promoted to Production.
